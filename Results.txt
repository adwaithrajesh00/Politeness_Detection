
[Text]
Features: data/processed/X_text.npy  shape=(1108, 384)
Labels:   data/raw/data.csv  n=1108
Using CSV 'split' → train=806, test=150

== Results ==
Accuracy: 0.9733

Classification Report:
              precision    recall  f1-score   support

 impolite(0)     1.0000    0.9467    0.9726        75
   polite(1)     0.9494    1.0000    0.9740        75

    accuracy                         0.9733       150
   macro avg     0.9747    0.9733    0.9733       150
weighted avg     0.9747    0.9733    0.9733       150

Confusion Matrix [rows=true, cols=pred]:
[[71  4]
 [ 0 75]]

Saved metrics → data\processed\metrics_single_run.json
(venv)

{{{{{Json}}}}}

{
  "feature_file": "data/processed/X_text.npy",
  "accuracy": 0.9733333333333334,
  "classification_report": "              precision    recall  f1-score   support\n\n impolite(0)     1.0000    0.9467    0.9726        75\n   polite(1)     0.9494    1.0000    0.9740        75\n\n    accuracy                         0.9733       150\n   macro avg     0.9747    0.9733    0.9733       150\nweighted avg     0.9747    0.9733    0.9733       150\n",
  "confusion_matrix": [
    [
      71,
      4
    ],
    [
      0,
      75
    ]
  ]
}

[Text + Emoji]

Features: data/processed/X_text_emoji.npy  shape=(1108, 687)
Labels:   data/raw/data.csv  n=1108
Using CSV 'split' → train=806, test=150

== Results ==
Accuracy: 0.9800

Classification Report:
              precision    recall  f1-score   support

 impolite(0)     1.0000    0.9600    0.9796        75
   polite(1)     0.9615    1.0000    0.9804        75

    accuracy                         0.9800       150
   macro avg     0.9808    0.9800    0.9800       150
weighted avg     0.9808    0.9800    0.9800       150

Confusion Matrix [rows=true, cols=pred]:
[[72  3]
 [ 0 75]]

Saved metrics → data\processed\metrics_single_run.json
(venv) 

{{{{{Json}}}}}

{
  "feature_file": "data/processed/X_text_emoji.npy",
  "accuracy": 0.98,
  "classification_report": "              precision    recall  f1-score   support\n\n impolite(0)     1.0000    0.9600    0.9796        75\n   polite(1)     0.9615    1.0000    0.9804        75\n\n    accuracy                         0.9800       150\n   macro avg     0.9808    0.9800    0.9800       150\nweighted avg     0.9808    0.9800    0.9800       150\n",
  "confusion_matrix": [
    [
      72,
      3
    ],
    [
      0,
      75
    ]
  ]
}

[Text + Emoji + Sticker]

Labels:   data/raw/data.csv  n=1108
Using CSV 'split' → train=806, test=150

== Results ==
Accuracy: 0.9800

Classification Report:
              precision    recall  f1-score   support

 impolite(0)     1.0000    0.9600    0.9796        75
   polite(1)     0.9615    1.0000    0.9804        75

    accuracy                         0.9800       150
   macro avg     0.9808    0.9800    0.9800       150
weighted avg     0.9808    0.9800    0.9800       150

Confusion Matrix [rows=true, cols=pred]:
[[72  3]
 [ 0 75]]

Saved metrics → data\processed\metrics_single_run.json
(venv) 

{{{{{Json}}}}}

{
  "feature_file": "data/processed/X_text_emoji_sticker.npy",
  "accuracy": 0.98,
  "classification_report": "              precision    recall  f1-score   support\n\n impolite(0)     1.0000    0.9600    0.9796        75\n   polite(1)     0.9615    1.0000    0.9804        75\n\n    accuracy                         0.9800       150\n   macro avg     0.9808    0.9800    0.9800       150\nweighted avg     0.9808    0.9800    0.9800       150\n",
  "confusion_matrix": [
    [
      72,
      3
    ],
    [
      0,
      75
    ]
  ]
}
